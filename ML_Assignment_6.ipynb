{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Gaussian NBC with the EM algorithm. \n",
    "Compare results with those of K-means clustering provided in Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The E-M algorithm:\n",
    "1. Compute the expected outcome for each example/sample given estimates for priors and distribution.\n",
    "2. Compute new estimates for priors and distributions bases on the estimated expected values for how much each sample belongs to the respective distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Viktor\\PycharmProjects\\ML_Assignment5\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\Viktor\\PycharmProjects\\ML_Assignment5\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and split the digits dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and split the digit dataset\n",
    "\n",
    "digit_dataset = datasets.load_digits()\n",
    "\n",
    "digits_x = digit_dataset.data\n",
    "digits_y = digit_dataset.target\n",
    "\n",
    "num_examples = len(digits_x)\n",
    "num_split = int(0.7*num_examples)\n",
    "\n",
    "#Split into training and test data\n",
    "digits_train_features = digits_x[:num_split]\n",
    "digits_train_labels =  digits_y[:num_split]\n",
    "digits_test_features = digits_x[num_split:]\n",
    "digits_test_labels = digits_y[num_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation of the E-M algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectationMaximisation:\n",
    "    \n",
    "    def __init__(self, features, k = 10):\n",
    "        \n",
    "        #Initialise pi_k, mu_k, sum_k where pi_k is class prior, mu_k is means for attribute value j in class k\n",
    "        self.CovK = 1\n",
    "        self.means = 1\n",
    "        self.pi_k = np.ones([10,1])*1/10\n",
    "        self.clustered = []\n",
    "\n",
    "        #Make random subsets for initialization\n",
    "        number_of_samples = len(features)\n",
    "        subset_size = number_of_samples/k\n",
    "        \n",
    "        for i in range(k):\n",
    "            self.clustered.append((features[round(i*subset_size):round((i+1)*subset_size)]))  \n",
    "        \n",
    "        #Find the number of unique values in features. 17 for digits, 3 for sumdigits\n",
    "        uniqueval = 0\n",
    "        uniquelist = []\n",
    "        for pic in features:\n",
    "            for pixel in pic:\n",
    "                if pixel not in uniquelist:\n",
    "                    uniquelist.append(pixel)\n",
    "        uniqueval = len(uniquelist)\n",
    "        \n",
    "        attributesums = np.ones([k,len(features[0])])*0.1\n",
    "        clustercount = np.zeros(k)\n",
    "        self.means = np.zeros([k,len(features[0])])  # Means for all features in cluster k\n",
    "        \n",
    "        valuecountmatrix = np.zeros([k,uniqueval,len(features[1])])\n",
    "        for k,cluster in enumerate(self.clustered): #Iterate through clusters\n",
    "            for p, pic in enumerate(cluster):  \n",
    "                clustercount[k] +=1            #Count number of pictures in the cluster\n",
    "                for l,pixel in enumerate(pic):\n",
    "                    valuecountmatrix[k][int(pixel*16)][l] +=1\n",
    "                    attributesums[k][l] += pixel  #Sum all pixel values for each feature\n",
    "        \n",
    "        for c, classrow in enumerate(attributesums):\n",
    "            for s, pixelsum in enumerate(classrow):\n",
    "                if clustercount[c] != 0:\n",
    "                    self.means[c][s] = pixelsum/clustercount[c]\n",
    "                else:\n",
    "                    self.means[c][s] = 0\n",
    "        \n",
    "        #For every class calculate variance\n",
    "        self.CovK = np.zeros([10,len(features[1])])\n",
    "        for cluster in range(10):\n",
    "            for i, pixelsums in enumerate(valuecountmatrix[cluster]):\n",
    "                for j,sum in enumerate(pixelsums):\n",
    "                    self.CovK[cluster][j] += j*(i-self.means[cluster][j])**2   \n",
    "                    \n",
    "        epsilon = 0.1\n",
    "        for cluster in range(10):\n",
    "            self.CovK[cluster] = (1/clustercount[cluster])*(self.CovK[cluster])+epsilon\n",
    "        return\n",
    "    \n",
    "        #End of initialization. CovK, Mu_k and Pi_k are thus created\n",
    "        \n",
    "    def fit(self, features, k = 10, r = 20):\n",
    "        \n",
    "        Clustered = False\n",
    "        #Start the iteration\n",
    "        count = 1\n",
    "        while Clustered == False:\n",
    "            print('Iteration: ' + str(count))\n",
    "            count+=1\n",
    "            \n",
    "            \n",
    "            r = np.zeros([len(features), k])\n",
    "            for i, picture in enumerate(features):\n",
    "                expected = np.zeros(k)\n",
    "                for cluster in range(10):\n",
    "                    dist = (picture-self.means[cluster])**2\n",
    "                    probj = (1/(np.sqrt(2*3.14159)*np.sqrt(self.CovK[cluster])))*np.exp(-(dist)/(2*self.CovK[cluster]))\n",
    "                    probj = np.prod(probj)*self.pi_k[cluster]\n",
    "                    expected[cluster] = probj\n",
    "                ri = expected/sum(expected)\n",
    "                r[i] = ri\n",
    "                \n",
    "            #rik has been computed for all images i and clusters k.\n",
    "\n",
    "            #M-step\n",
    "            N = len(features)\n",
    "            #Compute for each class the sum of all ri\n",
    "            rk = np.zeros(k)\n",
    "            for j in range(10):\n",
    "                for i in range(len(features)):      \n",
    "                    rk[j] +=  r[i][j]\n",
    "                self.pi_k[j] = rk[j]/N\n",
    "            #Compute for each class the new pi_k\n",
    "            \n",
    "            normsum = 0\n",
    "\n",
    "            #Update the means and the variances\n",
    "            for l in range(10):\n",
    "                mean = 0\n",
    "                Cov = 0       \n",
    "                for i in range(len(features)):\n",
    "                    Cov += r[i][l]*(features[i]*np.transpose(features[i]))\n",
    "                    mean += r[i][l]*features[i]\n",
    "                mean = mean/rk[l]\n",
    "                Cov = Cov/rk[l] - mean*np.transpose(mean) + 0.01\n",
    "                normsum += norm(self.means[l]-mean)\n",
    "                self.means[l] = mean\n",
    "                self.CovK[l] = Cov\n",
    "            print('Change in L2-norms: ' +str(normsum))\n",
    "            if normsum < 0.01:\n",
    "                Clustered = True\n",
    "                                    \n",
    "    def predict(self, data):\n",
    "        predicted = []\n",
    "        \n",
    "        #For every picture in the data:\n",
    "        for pictures in data:\n",
    "            pred = 1000\n",
    "            predicted_probability = 0\n",
    "            \n",
    "            for i in range(10):\n",
    "                cond_prob = 1\n",
    "                for p, pixel in enumerate(pictures):\n",
    "                        prob = (1/(np.sqrt(2*3.14159)*np.sqrt(self.CovK[i][p])))*np.exp(-((pixel-self.means[i][p])**2)/(2*self.CovK[i][p]))                               \n",
    "                        cond_prob = cond_prob*prob\n",
    "                prob_y = self.pi_k[i]*cond_prob\n",
    "                if prob_y > predicted_probability:\n",
    "                    predicted_probability = prob_y\n",
    "                    pred = i\n",
    "            predicted.append(pred)\n",
    "    \n",
    "        return predicted\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = digits_train_features/16\n",
    "EM = ExpectationMaximisation(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Change in L2-norms: 3.749708291060642\n",
      "Iteration: 2\n",
      "Change in L2-norms: 0.07000741203680746\n",
      "Iteration: 3\n",
      "Change in L2-norms: 0.669725726564204\n",
      "Iteration: 4\n",
      "Change in L2-norms: 3.598623658956015\n",
      "Iteration: 5\n",
      "Change in L2-norms: 5.219956224919757\n",
      "Iteration: 6\n",
      "Change in L2-norms: 4.476642467072828\n",
      "Iteration: 7\n",
      "Change in L2-norms: 2.61113947831729\n",
      "Iteration: 8\n",
      "Change in L2-norms: 1.3533971164913121\n",
      "Iteration: 9\n",
      "Change in L2-norms: 1.026729346064542\n",
      "Iteration: 10\n",
      "Change in L2-norms: 1.078182777861262\n",
      "Iteration: 11\n",
      "Change in L2-norms: 1.023293480299798\n",
      "Iteration: 12\n",
      "Change in L2-norms: 0.5978002865587844\n",
      "Iteration: 13\n",
      "Change in L2-norms: 0.49726937436171403\n",
      "Iteration: 14\n",
      "Change in L2-norms: 0.3808498101329075\n",
      "Iteration: 15\n",
      "Change in L2-norms: 0.35141365692727733\n",
      "Iteration: 16\n",
      "Change in L2-norms: 0.37982761438178325\n",
      "Iteration: 17\n",
      "Change in L2-norms: 0.25159335398324056\n",
      "Iteration: 18\n",
      "Change in L2-norms: 0.17565815669091037\n",
      "Iteration: 19\n",
      "Change in L2-norms: 0.14674672757255441\n",
      "Iteration: 20\n",
      "Change in L2-norms: 0.14569973425790364\n",
      "Iteration: 21\n",
      "Change in L2-norms: 0.09246675318075556\n",
      "Iteration: 22\n",
      "Change in L2-norms: 0.07814578793653836\n",
      "Iteration: 23\n",
      "Change in L2-norms: 0.07241249934249068\n",
      "Iteration: 24\n",
      "Change in L2-norms: 0.06170365491506129\n",
      "Iteration: 25\n",
      "Change in L2-norms: 0.050314298495959635\n",
      "Iteration: 26\n",
      "Change in L2-norms: 0.03518978177191037\n",
      "Iteration: 27\n",
      "Change in L2-norms: 0.030268137001478023\n",
      "Iteration: 28\n",
      "Change in L2-norms: 0.03139423075761171\n",
      "Iteration: 29\n",
      "Change in L2-norms: 0.02772403477986847\n",
      "Iteration: 30\n",
      "Change in L2-norms: 0.03043739200892193\n",
      "Iteration: 31\n",
      "Change in L2-norms: 0.035365113793532445\n",
      "Iteration: 32\n",
      "Change in L2-norms: 0.03976744454431843\n",
      "Iteration: 33\n",
      "Change in L2-norms: 0.07062186932515656\n",
      "Iteration: 34\n",
      "Change in L2-norms: 0.09926959755625073\n",
      "Iteration: 35\n",
      "Change in L2-norms: 0.13473584848457085\n",
      "Iteration: 36\n",
      "Change in L2-norms: 0.1692847362378929\n",
      "Iteration: 37\n",
      "Change in L2-norms: 0.1462425042286105\n",
      "Iteration: 38\n",
      "Change in L2-norms: 0.16307668494263888\n",
      "Iteration: 39\n",
      "Change in L2-norms: 0.18606684290320583\n",
      "Iteration: 40\n",
      "Change in L2-norms: 0.15767567318357936\n",
      "Iteration: 41\n",
      "Change in L2-norms: 0.0950581369268214\n",
      "Iteration: 42\n",
      "Change in L2-norms: 0.09816509125582779\n",
      "Iteration: 43\n",
      "Change in L2-norms: 0.11480032024941318\n",
      "Iteration: 44\n",
      "Change in L2-norms: 0.1417391630922266\n",
      "Iteration: 45\n",
      "Change in L2-norms: 0.11791629844489958\n",
      "Iteration: 46\n",
      "Change in L2-norms: 0.09186184906683485\n",
      "Iteration: 47\n",
      "Change in L2-norms: 0.08415536676834484\n",
      "Iteration: 48\n",
      "Change in L2-norms: 0.08819087571163027\n",
      "Iteration: 49\n",
      "Change in L2-norms: 0.08943714458004312\n",
      "Iteration: 50\n",
      "Change in L2-norms: 0.08287090217268926\n",
      "Iteration: 51\n",
      "Change in L2-norms: 0.08931236893117954\n",
      "Iteration: 52\n",
      "Change in L2-norms: 0.10534393274604918\n",
      "Iteration: 53\n",
      "Change in L2-norms: 0.1238746575331097\n",
      "Iteration: 54\n",
      "Change in L2-norms: 0.10785637355094298\n",
      "Iteration: 55\n",
      "Change in L2-norms: 0.10052948505164011\n",
      "Iteration: 56\n",
      "Change in L2-norms: 0.11168102484193651\n",
      "Iteration: 57\n",
      "Change in L2-norms: 0.08873348383485728\n",
      "Iteration: 58\n",
      "Change in L2-norms: 0.07100859610147763\n",
      "Iteration: 59\n",
      "Change in L2-norms: 0.05289621518181001\n",
      "Iteration: 60\n",
      "Change in L2-norms: 0.04488333822514907\n",
      "Iteration: 61\n",
      "Change in L2-norms: 0.03765860739812282\n",
      "Iteration: 62\n",
      "Change in L2-norms: 0.023383442023943733\n",
      "Iteration: 63\n",
      "Change in L2-norms: 0.018252960471100168\n",
      "Iteration: 64\n",
      "Change in L2-norms: 0.007741459995017714\n"
     ]
    }
   ],
   "source": [
    "EM.fit(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " pred = EM.predict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of results and comparison with Sklearn KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKlearn: \n",
      "Confusion matrix\n",
      "\n",
      "[[  0   0   0 125   0   0   0   0   0   0]\n",
      " [  2  63   0   0   0  25   0   0   0  39]\n",
      " [  0   8   2   0   2 108   0   0   1   3]\n",
      " [  0   0 110   0   2   0   0   2  16   0]\n",
      " [  0   2   0   0   7   0 109   0   0   6]\n",
      " [  1   0   1   0   0   0   1  94  29   0]\n",
      " [124   2   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0 122   0   0   1   0   2]\n",
      " [  1  61   1   0   1   2   0   8  44   4]\n",
      " [  0   0   3   0   7   0   0   3  97  15]]\n",
      "\n",
      "Completeness score against ground truth: 0.7551388129562558\n",
      "Homogeneity score against ground truth: 0.7466873334922608\n",
      "Adjusted mutual info against ground truth: 0.747266298787389\n",
      "\n",
      "EM-Algorithm\n",
      "\n",
      "Confusion matrix\n",
      "[[  0   0   1   1 123   0   0   0   0   0]\n",
      " [ 65   0   0   0   0  37   0  27   0   0]\n",
      " [ 13   0   0   0   0   1   0  94  16   0]\n",
      " [  0   0  17   0   0   1   3   3 106   0]\n",
      " [  0   0   0 110   0   5   8   0   0   1]\n",
      " [  0   0  31   1   0   0   1   0   1  92]\n",
      " [  2 122   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  17 108   0   0   0]\n",
      " [ 91   0  23   0   0   5   0   2   0   1]\n",
      " [  2   0  98   0   0  17   6   0   2   0]]\n",
      "\n",
      "Completeness score against ground truth: 0.7469824506530364\n",
      "Homogeneity score against ground truth: 0.7394461102847253\n",
      "Adjusted mutual info against ground truth: 0.7394636303479535\n",
      "\n",
      "EM against Kmeans:\n",
      "Completeness score against Kmeans 0.8335829517998284\n",
      "Homogeneity score against Kmeans: 0.8326542158346942\n",
      "Adjusted mutual info against Kmeans: 0.8306757408938843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clustering = KMeans(n_clusters=10)\n",
    "clusters = clustering.fit(d)\n",
    "\n",
    "prediction = clustering.predict(d)\n",
    "\n",
    "print('SKlearn: ')\n",
    "print('Confusion matrix')\n",
    "print()\n",
    "print(metrics.confusion_matrix(digits_train_labels, prediction))\n",
    "print()\n",
    "print('Completeness score against ground truth: ' + str(metrics.completeness_score(digits_train_labels, prediction)))\n",
    "print('Homogeneity score against ground truth: ' + str(metrics.homogeneity_score(digits_train_labels, prediction)))\n",
    "print('Adjusted mutual info against ground truth: ' + str(metrics.adjusted_mutual_info_score(digits_train_labels, prediction)))\n",
    "print()\n",
    "print('EM-Algorithm')\n",
    "print()\n",
    "print('Confusion matrix')\n",
    "print(metrics.confusion_matrix(digits_train_labels, pred))\n",
    "print()\n",
    "print('Completeness score against ground truth: ' + str(metrics.completeness_score(digits_train_labels, pred)))\n",
    "print('Homogeneity score against ground truth: ' + str(metrics.homogeneity_score(digits_train_labels, pred)))\n",
    "print('Adjusted mutual info against ground truth: ' + str(metrics.adjusted_mutual_info_score(digits_train_labels, pred)))\n",
    "print()\n",
    "print('EM against Kmeans:')\n",
    "print('Completeness score against Kmeans ' + str(metrics.completeness_score(pred, prediction)))\n",
    "print('Homogeneity score against Kmeans: ' + str(metrics.homogeneity_score(pred, prediction)))\n",
    "print('Adjusted mutual info against Kmeans: ' + str(metrics.adjusted_mutual_info_score(pred, prediction)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discarded code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExpectationMaximisation:\n",
    "    \n",
    "    def __init__(self, features, k = 10):\n",
    "        \n",
    "        #Initialise pi_k, mu_k, sum_k where pi_k is class prior, mu_k is means for attribute value j in class k\n",
    "        self.CovK = 1\n",
    "        self.means = 1\n",
    "        self.pi_k = np.ones([10,1])*1/10\n",
    "        self.clustered = []\n",
    "\n",
    "        #Make random subsets for initialization\n",
    "        number_of_samples = len(features)\n",
    "        subset_size = number_of_samples/k\n",
    "        \n",
    "        for i in range(k):\n",
    "            self.clustered.append((features[round(i*subset_size):round((i+1)*subset_size)]))  \n",
    "        \n",
    "        #Find the number of unique values in features. 17 for digits, 3 for sumdigits\n",
    "        uniqueval = 0\n",
    "        uniquelist = []\n",
    "        for pic in features:\n",
    "            for pixel in pic:\n",
    "                if pixel not in uniquelist:\n",
    "                    uniquelist.append(pixel)\n",
    "        uniqueval = len(uniquelist)\n",
    "        \n",
    "        attributesums = np.ones([k,len(features[0])])*0.1\n",
    "        clustercount = np.zeros(k)\n",
    "        self.means = np.zeros([k,len(features[0])])  # Means for all features in cluster k\n",
    "        \n",
    "        valuecountmatrix = np.zeros([k,uniqueval,len(features[1])])\n",
    "        for k,cluster in enumerate(self.clustered): #Iterate through clusters\n",
    "            for p, pic in enumerate(cluster):  \n",
    "                clustercount[k] +=1            #Count number of pictures in the cluster\n",
    "                for l,pixel in enumerate(pic):\n",
    "                    valuecountmatrix[k][int(pixel)][l] +=1\n",
    "                    attributesums[k][l] += pixel  #Sum all pixel values for each feature\n",
    "        \n",
    "        for c, classrow in enumerate(attributesums):\n",
    "            for s, pixelsum in enumerate(classrow):\n",
    "                if clustercount[c] != 0:\n",
    "                    self.means[c][s] = pixelsum/clustercount[c]\n",
    "                else:\n",
    "                    self.means[c][s] = 0\n",
    "        \n",
    "        #For every class calculate variance\n",
    "        self.CovK = np.zeros([10,len(features[1])])\n",
    "        for cluster in range(10):\n",
    "            for i, pixelsums in enumerate(valuecountmatrix[cluster]):\n",
    "                for j,sum in enumerate(pixelsums):\n",
    "                    self.CovK[cluster][j] += j*(i-self.means[cluster][j])**2   \n",
    "                    \n",
    "        epsilon = 0.1\n",
    "        for cluster in range(10):\n",
    "            self.CovK[cluster] = (1/clustercount[cluster])*(self.CovK[cluster])+epsilon\n",
    "        return\n",
    "    \n",
    "        #End of initialization. CovK, Mu_k and Pi_k are thus created\n",
    "        \n",
    "    def fit(self, features, k = 10, r = 20):\n",
    "        \n",
    "        #Start the iteration\n",
    "        \n",
    "        for r in range(r):\n",
    "            print('BEGINNING ITERATION: ' + str(r))\n",
    "            \n",
    "            \n",
    "            #E-step.\n",
    "            #For every picture in the data compute rik\n",
    "            #rik is probability of picture i belonging to cluster k assuming feature distributions j for kluster k,\n",
    "            #divided by sum for all clusters\n",
    "            r = np.zeros([len(features), k])\n",
    "            for i,picture in enumerate(features):\n",
    "                expected = np.zeros(k)\n",
    "                for cluster in range(k):\n",
    "                    nominator = self.pi_k[cluster]\n",
    "                    for j, pixel in enumerate(picture):\n",
    "                        prob = (1/(np.sqrt(2*3.14159)*np.sqrt(self.CovK[cluster][j])))*math.exp(-((pixel-self.means[cluster][j])**2)/(2*self.CovK[cluster][j]))                                  \n",
    "                        nominator = nominator*prob\n",
    "                    expected[cluster] = nominator\n",
    "                ri = expected/sum(expected)\n",
    "                r[i] = ri\n",
    "            #rik has been computed for all images i and clusters k.\n",
    "            print('E-STEP DONE')\n",
    "\n",
    "            #M-step\n",
    "            N = len(features)\n",
    "            #Compute for each class the sum of all ri\n",
    "            rk = np.zeros(k)\n",
    "            for j in range(10):\n",
    "                for i in range(len(features)):      \n",
    "                    rk[j] +=  r[i][j]\n",
    "                self.pi_k[j] = rk[j]/N\n",
    "            #Compute for each class the new pi_k\n",
    "\n",
    "            #Update the means and the variances\n",
    "            for l in range(10):\n",
    "                mean = 0\n",
    "                Cov = 0       \n",
    "                for i in range(len(features)):\n",
    "                    Cov += r[i][l]*(features[i]*np.transpose(features[i]))\n",
    "                    mean += r[i][l]*features[i]\n",
    "                mean = mean/rk[l]\n",
    "                Cov = Cov/rk[l] - mean*np.transpose(mean) + 0.01\n",
    "                self.means[l] = mean\n",
    "                self.CovK[l] = Cov\n",
    "            print('M-STEP DONE')\n",
    "                                    \n",
    "    def predict(self, data):\n",
    "        predicted = []\n",
    "        \n",
    "        #For every picture in the data:\n",
    "        for pictures in data:\n",
    "            pred = 1000\n",
    "            predicted_probability = 0\n",
    "            \n",
    "            for i in range(10):\n",
    "                cond_prob = 1\n",
    "                for p, pixel in enumerate(pictures):\n",
    "                        prob = (1/(np.sqrt(2*3.14159)*np.sqrt(self.CovK[i][p])))*math.exp(-((pixel-self.means[i][p])**2)/(2*self.CovK[i][p]))                               \n",
    "                        cond_prob = cond_prob*prob\n",
    "                prob_y = self.pi_k[i]*cond_prob\n",
    "                if prob_y > predicted_probability:\n",
    "                    predicted_probability = prob_y\n",
    "                    pred = i\n",
    "            predicted.append(pred)\n",
    "    \n",
    "        return predicted\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
